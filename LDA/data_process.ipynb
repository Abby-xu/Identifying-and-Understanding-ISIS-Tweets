{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e635d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import spacy\n",
    "import gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mpu\n",
    "\n",
    "np.set_printoptions(threshold= sys.maxsize)\n",
    "np.random.seed(5)\n",
    "\n",
    "tweet_file_name = 'tweets.xlsx'\n",
    "\n",
    "subtitle_dic = {}\n",
    "def read_csv(csv_name):\n",
    "\twith open(csv_name, newline='', encoding=\"utf8\") as f:\n",
    "\t\treader = csv.reader(f)\n",
    "\t\ttmp = list(reader)\n",
    "\treturn tmp\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "def lemmatization(doc, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "    texts_out = []\n",
    "    doc = nlp(doc)\n",
    "    texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('english translation')\n",
    "stop_words.add('@')\n",
    "def text_processing(raw_tweet):\n",
    "\tword_tokens = word_tokenize(raw_tweet)\n",
    "\tscript = [w for w in word_tokens if not w in stop_words]\n",
    "\tscript = TreebankWordDetokenizer().detokenize(script)\n",
    "\tscript = list(gensim.utils.simple_preprocess(str(script), deacc=False))\n",
    "\tscript = \" \".join(script)\n",
    "\tscript = lemmatization(script)\n",
    "\treturn script\n",
    "\n",
    "tweets = pd.read_excel(tweet_file_name)\n",
    "user_tweets_dic = {}\n",
    "for index, row in tweets.iterrows():\n",
    "\tuser_tmp = row['username']\n",
    "\ttweets_tmp = row['tweets']\n",
    "\tif (tweets_tmp.upper() != tweets_tmp.lower()):\n",
    "\t\tif (user_tmp not in user_tweets_dic):\n",
    "\t\t\tuser_tweets_dic[user_tmp] = text_processing(tweets_tmp)\n",
    "\t\telse:\n",
    "\t\t\tuser_tweets_dic[user_tmp] = user_tweets_dic[user_tmp] + text_processing(tweets_tmp)\n",
    "\n",
    "\n",
    "\n",
    "# for key in user_tweets_dic.keys():\n",
    "# \tprint(key, len(user_tweets_dic[key]))\n",
    "\n",
    "mpu.io.write('user_tweets_dic.pickle', user_tweets_dic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
