{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import spacy\n",
    "import gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mpu\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(threshold= sys.maxsize)\n",
    "np.random.seed(5)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# train lda\n",
    "# --------------------------------------------------------------------------------\n",
    "load_model = True\n",
    "save_to_model = False\n",
    "model_name = 'saved_model/model_0.pkl'\n",
    "topic_number = 5\n",
    "max_df = 0.8\n",
    "min_df = 0.01\n",
    "\n",
    "user_tweets_dic = mpu.io.read('user_tweets_dic.pickle')\n",
    "\n",
    "train_data = []\n",
    "for key in sorted(user_tweets_dic.keys()):\n",
    "\ttrain_data = train_data + user_tweets_dic[key]\n",
    "\n",
    "\n",
    "if load_model:\n",
    "\twith open(model_name, 'rb') as f:\n",
    "\t    vectorizer, lda_model = pickle.load(f)\n",
    "\tdata_vectorized = vectorizer.transform(train_data)\n",
    "\tlda_output = lda_model.transform(data_vectorized)\n",
    "else:\n",
    "\tvectorizer = CountVectorizer(max_df = max_df, min_df=min_df)\n",
    "\tdata_vectorized = vectorizer.fit_transform(train_data)\n",
    "\n",
    "\tlda_model = LatentDirichletAllocation(n_components=topic_number, learning_method='online', random_state=100, max_iter=100)\n",
    "\tlda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# evaluate lda\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "score_perplexity = lda_model.perplexity(data_vectorized)\n",
    "print(\"perplexity:\", score_perplexity)\n",
    "\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "topic_word_distrib = np.array(lda_model.components_)\n",
    "dtm = vectorizer.transform(train_data)\n",
    "\n",
    "score_coherence = metric_coherence_gensim(measure='u_mass', topic_word_distrib=topic_word_distrib, \n",
    "\t\tvocab=vocab, dtm=dtm, return_mean=True)\n",
    "print(\"coherence:\", score_coherence)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# get topic scores for lda and save model if needed\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "if save_to_model:\n",
    "\twith open(model_name, 'wb') as f:\n",
    "\t    pickle.dump((vectorizer, lda_model), f)\n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, verbose=True, n_words=20):\n",
    "\tkeywords = np.array(vectorizer.get_feature_names())\n",
    "\ttopic_keywords = []\n",
    "\tfor topic_weights in lda_model.components_:\n",
    "\t\ttop_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "\t\ttopic_keywords.append(keywords.take(top_keyword_locs))\n",
    "\tif verbose:\n",
    "\t\ttotal = []\n",
    "\t\tfor i in range(0, len(topic_keywords)):\n",
    "\t\t\tprint(\"Topic \" + str(i), end = \" \")\n",
    "\t\t\tprint(list(topic_keywords[i]))\n",
    "\t\t\ttotal = total + list(topic_keywords[i])\n",
    "\t\tprint(len(list(set(total))))\n",
    "\t\texit()\n",
    "\treturn topic_keywords\n",
    "\n",
    "show_topics(vectorizer, lda_model)\n",
    "dominant_topic = np.argmax(lda_output, axis=1)\n",
    "print(Counter(dominant_topic))\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# get user topic distribution\n",
    "# --------------------------------------------------------------------------------\n",
    "# print('user', 'total_tweets', 'topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4')\n",
    "# usernames = []\n",
    "# user_topic_count = {}\n",
    "# for key in sorted(user_tweets_dic.keys()):\n",
    "# \t# train_data = train_data + user_tweets_dic[key]\n",
    "# \tusernames = usernames + [key] * len(user_tweets_dic[key])\n",
    "# \tuser_topic_count[key] = {0:0, 1:0, 2:0, 3:0, 4:0}\n",
    "\n",
    "# for i in range(0, len(usernames)):\n",
    "# \tuser_topic_count[usernames[i]][dominant_topic[i]] += 1\n",
    "\n",
    "\n",
    "# for key in user_topic_count.keys():\n",
    "# \ttmp = user_topic_count[key]\n",
    "# \ts = float(tmp[0] + tmp[1] + tmp[2]+ tmp[3] + tmp[4])\n",
    "# \tprint(key, s, tmp[0]/s, tmp[1]/s, tmp[2]/s, tmp[3]/s, tmp[4]/s)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
